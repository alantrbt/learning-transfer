\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2023)Ainsworth, Hayase, and Srinivasa]{ainsworth2023git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=CQsmMYmlP5T}.

\bibitem[Ash \& Adams(2020)Ash and Adams]{ash2020warm}
Jordan Ash and Ryan~P Adams.
\newblock On warm-starting neural network training.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 3884--3894, 2020.

\bibitem[Benton et~al.(2021)Benton, Maddox, Lotfi, and Wilson]{benton2021loss}
Gregory Benton, Wesley Maddox, Sanae Lotfi, and Andrew Gordon~Gordon Wilson.
\newblock Loss surface simplexes for mode connecting volumes and fast ensembling.
\newblock In \emph{International Conference on Machine Learning}, pp.\  769--779. PMLR, 2021.

\bibitem[Benzing et~al.(2022)Benzing, Schug, Meier, Von~Oswald, Akram, Zucchet, Aitchison, and Steger]{benzing2022random}
Frederik Benzing, Simon Schug, Robert Meier, Johannes Von~Oswald, Yassir Akram, Nicolas Zucchet, Laurence Aitchison, and Angelika Steger.
\newblock Random initialisations performing above chance and how to find them.
\newblock In \emph{OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)}, 2022.

\bibitem[Bhojanapalli et~al.(2021)Bhojanapalli, Wilber, Veit, Rawat, Kim, Menon, and Kumar]{bhojanapalli2021reproducibility}
Srinadh Bhojanapalli, Kimberly Wilber, Andreas Veit, Ankit~Singh Rawat, Seungyeon Kim, Aditya Menon, and Sanjiv Kumar.
\newblock On the reproducibility of neural network predictions.
\newblock \emph{arXiv preprint arXiv:2102.03349}, 2021.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and LeCun]{choromanska2015loss}
Anna Choromanska, Mikael Henaff, Michael Mathieu, G{\'e}rard~Ben Arous, and Yann LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  192--204. PMLR, 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and Hamprecht]{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{International conference on machine learning}, pp.\  1309--1318. PMLR, 2018.

\bibitem[Entezari et~al.(2021)Entezari, Sedghi, Saukh, and Neyshabur]{entezari2021role}
Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur.
\newblock The role of permutation invariance in linear mode connectivity of neural networks.
\newblock \emph{arXiv preprint arXiv:2110.06296}, 2021.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{arXiv preprint arXiv:1912.02757}, 2019.

\bibitem[Frankle(2020)]{frankle2020revisiting}
Jonathan Frankle.
\newblock Revisiting ''qualitatively characterizing neural network optimization problems''.
\newblock In \emph{NeurIPS 2020 Workshop: Deep Learning through Information Geometry}, 2020.
\newblock URL \url{https://openreview.net/forum?id=0mu8aLQ3Kyc}.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3259--3269. PMLR, 2020.

\bibitem[Freeman \& Bruna(2017)Freeman and Bruna]{freeman2017topology}
C.~Daniel Freeman and Joan Bruna.
\newblock Topology and geometry of half-rectified network optimization.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Bk0FWVcgx}.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson]{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P Vetrov, and Andrew~G Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Vinyals, and Saxe]{goodfellow2015qualitatively}
Ian~J. Goodfellow, Oriol Vinyals, and Andrew~M. Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Haeffele \& Vidal(2017)Haeffele and Vidal]{haeffele2017global}
Benjamin~D Haeffele and Ren{\'e} Vidal.
\newblock Global optimality in neural network training.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  7331--7339, 2017.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Huang et~al.(2017)Huang, Li, Pleiss, Liu, Hopcroft, and Weinberger]{huang2017snapshot}
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John~E. Hopcroft, and Kilian~Q. Weinberger.
\newblock Snapshot ensembles: Train 1, get m for free.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=BJYwwY9ll}.

\bibitem[Ilharco et~al.(2022)Ilharco, Wortsman, Gadre, Song, Hajishirzi, Kornblith, Farhadi, and Schmidt]{ilharco2022patching}
Gabriel Ilharco, Mitchell Wortsman, Samir~Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt.
\newblock Patching open-vocabulary models by interpolating weights.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=CZZFRxbOLC}.

\bibitem[Ilharco et~al.(2023)Ilharco, Ribeiro, Wortsman, Schmidt, Hajishirzi, and Farhadi]{ilharco2023editing}
Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=6t0Kwf8-jrj}.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\  448--456. pmlr, 2015.

\bibitem[Jordan(2023)]{jordan2023calibrated}
Keller Jordan.
\newblock Calibrated chaos: Variance between runs of neural network training is harmless and inevitable.
\newblock \emph{arXiv preprint arXiv:2304.01910}, 2023.

\bibitem[Jordan et~al.(2023)Jordan, Sedghi, Saukh, Entezari, and Neyshabur]{jordan2023repair}
Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur.
\newblock {REPAIR}: {RE}normalizing permuted activations for interpolation repair.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=gU5sJ6ZggcX}.

\bibitem[Juneja et~al.(2023)Juneja, Bansal, Cho, Sedoc, and Saphra]{juneja2023linear}
Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Jo{\~a}o Sedoc, and Naomi Saphra.
\newblock Linear connectivity reveals generalization strategies.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=hY6M0JHl3uL}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang]{keskar2017onlargebatch}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1oyRlYgg}.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{krause20133d}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision workshops}, pp.\  554--561, 2013.

\bibitem[Krizhevsky(2009)]{krizhevsky2009cifar10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images, 2009.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Kuhn(1955)]{kuhn1955hungarian}
Harold~W Kuhn.
\newblock The hungarian method for the assignment problem.
\newblock \emph{Naval research logistics quarterly}, 2\penalty0 (1-2):\penalty0 83--97, 1955.

\bibitem[LeCun et~al.(1998)LeCun, Cortes, Burges, et~al.]{lecun2010mnist}
Yann LeCun, Corinna Cortes, Chris Burges, et~al.
\newblock Mnist handwritten digit database, 1998.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016gradient}
Jason~D Lee, Max Simchowitz, Michael~I Jordan, and Benjamin Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{Conference on learning theory}, pp.\  1246--1257. PMLR, 2016.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li et~al.(2023)Li, Gururangan, Dettmers, Lewis, Althoff, Smith, and Zettlemoyer]{li2023branchtrainmerge}
Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah~A. Smith, and Luke Zettlemoyer.
\newblock Branch-train-merge: Embarrassingly parallel training of expert language models, 2023.
\newblock URL \url{https://openreview.net/forum?id=I8ly64E5Nt}.

\bibitem[Li \& Yuan(2017)Li and Yuan]{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu activation.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Liu et~al.(2022)Liu, Leonardi, Yu, Gilmer-Hill, Leavitt, and Frankle]{liu2022knowledge}
Xingyu Liu, Alexander Leonardi, Lu~Yu, Christopher Gilmer-Hill, Matthew~L Leavitt, and Jonathan Frankle.
\newblock Knowledge distillation for efficient sequences of training runs.
\newblock In \emph{First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022}, 2022.
\newblock URL \url{https://openreview.net/forum?id=kksQ0J87f03}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Skq89Scxx}.

\bibitem[Lubana et~al.(2023)Lubana, Bigelow, Dick, Krueger, and Tanaka]{lubana2023mechanistic}
Ekdeep~Singh Lubana, Eric~J Bigelow, Robert~P Dick, David Krueger, and Hidenori Tanaka.
\newblock Mechanistic mode connectivity.
\newblock 2023.

\bibitem[Lucas et~al.(2021)Lucas, Bae, Zhang, Fort, Zemel, and Grosse]{lucas2021onmonotonic}
James~R Lucas, Juhan Bae, Michael~R Zhang, Stanislav Fort, Richard Zemel, and Roger~B Grosse.
\newblock On monotonic linear interpolation of neural network parameters.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7168--7179. PMLR, 2021.

\bibitem[Matena \& Raffel(2022)Matena and Raffel]{matena2022merging}
Michael~S Matena and Colin~A Raffel.
\newblock Merging models with fisher-weighted averaging.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 17703--17716, 2022.

\bibitem[Mirzadeh et~al.(2021)Mirzadeh, Farajtabar, Gorur, Pascanu, and Ghasemzadeh]{mirzadeh2021linear}
Seyed~Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan Ghasemzadeh.
\newblock Linear mode connectivity in multitask and continual learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Fmg_fQYUejf}.

\bibitem[Nagarajan \& Kolter(2019)Nagarajan and Kolter]{nagarajan2019uniform}
Vaishnavh Nagarajan and J~Zico Kolter.
\newblock Uniform convergence may be unable to explain generalization in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and Zhang]{neyshabur2020being}
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.
\newblock What is being transferred in transfer learning?
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 512--523, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Santurkar et~al.(2021)Santurkar, Tsipras, Elango, Bau, Torralba, and Madry]{santurkar2021editing}
Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry.
\newblock Editing a classifier by rewriting its prediction rules.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 23359--23373, 2021.

\bibitem[Simsek et~al.(2021)Simsek, Ged, Jacot, Spadaro, Hongler, Gerstner, and Brea]{simsek2021geometry}
Berfin Simsek, Fran{\c{c}}ois Ged, Arthur Jacot, Francesco Spadaro, Cl{\'e}ment Hongler, Wulfram Gerstner, and Johanni Brea.
\newblock Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9722--9732. PMLR, 2021.

\bibitem[Singh \& Jaggi(2020)Singh and Jaggi]{singh2020model}
Sidak~Pal Singh and Martin Jaggi.
\newblock Model fusion via optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 22045--22055, 2020.

\bibitem[Sinitsin et~al.(2020)Sinitsin, Plokhotnyuk, Pyrkin, Popov, and Babenko]{sinitsin2020editable}
Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and Artem Babenko.
\newblock Editable neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HJedXaEtvS}.

\bibitem[Summers \& Dinneen(2021)Summers and Dinneen]{summers2021nondeterminism}
Cecilia Summers and Michael~J Dinneen.
\newblock Nondeterminism and instability in neural network optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9913--9922. PMLR, 2021.

\bibitem[Vlaar \& Frankle(2022)Vlaar and Frankle]{vlaar2022can}
Tiffany~J Vlaar and Jonathan Frankle.
\newblock What can linear interpolation of neural network loss landscapes tell us?
\newblock In \emph{International Conference on Machine Learning}, pp.\  22325--22341. PMLR, 2022.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and Belongie]{wah2011caltech}
C.~Wah, S.~Branson, P.~Welinder, P.~Perona, and S.~Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock \penalty0 (CNS-TR-2011-001), 2011.

\bibitem[Wang et~al.(2023)Wang, Wang, Zhou, and Ge]{wang2023plateau}
Xiang Wang, Annie~N. Wang, Mo~Zhou, and Rong Ge.
\newblock Plateau in monotonic linear interpolation --- a ''biased'' view of loss landscape for deep networks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=z289SIQOQna}.

\bibitem[Wortsman et~al.(2021)Wortsman, Horton, Guestrin, Farhadi, and Rastegari]{wortsman2021learning}
Mitchell Wortsman, Maxwell~C Horton, Carlos Guestrin, Ali Farhadi, and Mohammad Rastegari.
\newblock Learning neural network subspaces.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11217--11227. PMLR, 2021.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.
\newblock In \emph{International Conference on Machine Learning}, pp.\  23965--23998. PMLR, 2022.

\bibitem[Yin et~al.(2021)Yin, Mallya, Vahdat, Alvarez, Kautz, and Molchanov]{yin2021see}
Hongxu Yin, Arun Mallya, Arash Vahdat, Jose~M Alvarez, Jan Kautz, and Pavlo Molchanov.
\newblock See through gradients: Image batch recovery via gradinversion.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  16337--16346, 2021.

\bibitem[Yun et~al.(2018)Yun, Sra, and Jadbabaie]{yun2018global}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Global optimality conditions for deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJk7Gf-CZ}.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{zhao2021dataset}
Bo~Zhao, Konda~Reddy Mopuri, and Hakan Bilen.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=mSAKhLYLSsl}.

\end{thebibliography}
